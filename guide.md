# PagedAttentionå®žéªŒè¯¦ç»†æ­¥éª¤è§£æž

## ç¬¬ä¸€æ­¥ï¼šåˆ›å»ºå®žéªŒç›®å½•
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
ä¸ºå®žéªŒåˆ›å»ºä¸€ä¸ªä¸“é—¨çš„æ–‡ä»¶å¤¹ï¼Œé¿å…å’Œå…¶ä»–é¡¹ç›®æ··åœ¨ä¸€èµ·

### ðŸ“‚ å…·ä½“æ“ä½œï¼š
```bash
# ç¡®è®¤å½“å‰ä½ç½®
pwd
# 
# åˆ›å»ºå®žéªŒæ–‡ä»¶å¤¹
mkdir paged-attention-test
# ðŸ‘€ ä½œç”¨ï¼šåˆ›å»ºåä¸º "paged-attention-test" çš„æ–°æ–‡ä»¶å¤¹

cd paged-attention-test
# ðŸ‘€ ä½œç”¨ï¼šè¿›å…¥åˆšåˆ›å»ºçš„å®žéªŒæ–‡ä»¶å¤¹

# åˆ›å»ºå­ç›®å½•
mkdir scripts logs results
# ðŸ‘€ ä½œç”¨ï¼š
#   â€¢ scripts - å­˜æ”¾Pythonè„šæœ¬æ–‡ä»¶
#   â€¢ logs - å­˜æ”¾è¿è¡Œæ—¥å¿—
#   â€¢ results - å­˜æ”¾å®žéªŒç»“æžœ

ls -la
# ðŸ‘€ ä½œç”¨ï¼šæŸ¥çœ‹åˆ›å»ºçš„ç›®å½•ç»“æž„ï¼Œç¡®è®¤éƒ½åˆ›å»ºæˆåŠŸäº†
```

### ðŸ—‚ï¸ æœ€ç»ˆç›®å½•ç»“æž„ï¼š
```
paged-attention-test/
â”œâ”€â”€ scripts/     (Pythonè„šæœ¬)
â”œâ”€â”€ logs/        (æ—¥å¿—æ–‡ä»¶)
â””â”€â”€ results/     (ç»“æžœæ•°æ®)
```

---

## ç¬¬äºŒæ­¥ï¼šåˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„PythonçŽ¯å¢ƒï¼Œé¿å…å½±å“ä½ ç³»ç»Ÿä¸­å·²æœ‰çš„PythonåŒ…

### ðŸ ä¸ºä»€ä¹ˆéœ€è¦è™šæ‹ŸçŽ¯å¢ƒï¼š
- **éš”ç¦»æ€§**ï¼šå®žéªŒç”¨çš„åŒ…ä¸ä¼šå½±å“ç³»ç»ŸPython
- **ç‰ˆæœ¬æŽ§åˆ¶**ï¼šå¯ä»¥å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„åŒ…
- **æ¸…æ´æ€§**ï¼šå®žéªŒç»“æŸåŽå¯ä»¥ç›´æŽ¥åˆ é™¤ï¼Œä¸ç•™ç—•è¿¹

### ðŸ“‚ å…·ä½“æ“ä½œï¼š
```bash
# åˆ›å»ºPythonè™šæ‹ŸçŽ¯å¢ƒ
python3 -m venv venv-paged-test
# ðŸ‘€ ä½œç”¨ï¼š
#   â€¢ ä½¿ç”¨ä½ çš„Python 3.12åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
#   â€¢ åå­—å« "venv-paged-test"
#   â€¢ ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«ç‹¬ç«‹Pythonè§£é‡Šå™¨çš„æ–‡ä»¶å¤¹

# æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
source venv-paged-test/bin/activate
# ðŸ‘€ ä½œç”¨ï¼š
#   â€¢ å¯åŠ¨è™šæ‹ŸçŽ¯å¢ƒ
#   â€¢ ä¹‹åŽå®‰è£…çš„åŒ…åªä¼šè¿›å…¥è¿™ä¸ªçŽ¯å¢ƒ
#   â€¢ å‘½ä»¤è¡Œå‰é¢ä¼šæ˜¾ç¤º (venv-paged-test)

# ç¡®è®¤æ¿€æ´»æˆåŠŸ
which python
# ðŸ‘€ ä½œç”¨ï¼šæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„Pythonè·¯å¾„ï¼Œåº”è¯¥æŒ‡å‘è™šæ‹ŸçŽ¯å¢ƒ

python --version
# ðŸ‘€ ä½œç”¨ï¼šç¡®è®¤Pythonç‰ˆæœ¬è¿˜æ˜¯3.12.3
```

### ðŸ” éªŒè¯æ•ˆæžœï¼š
æ¿€æ´»åŽï¼Œå‘½ä»¤è¡Œåº”è¯¥å˜æˆï¼š
```
(venv-paged-test) cxxxxx@ubuntu:~/vllm-learning/experiments/paged-attention-test$
```

---

## ç¬¬ä¸‰æ­¥ï¼šå®‰è£…ç¼ºå¤±çš„åŒ…
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­å®‰è£…å®žéªŒéœ€è¦çš„PythonåŒ…

### ðŸ“¦ éœ€è¦å®‰è£…çš„åŒ…ï¼š
- **transformers**ï¼šä¼ ç»Ÿçš„å¤§æ¨¡åž‹æŽ¨ç†åº“
- **vLLM**ï¼šä½¿ç”¨PagedAttentionçš„é«˜æ•ˆæŽ¨ç†åº“
- **nvidia-ml-py3**ï¼šGPUç›‘æŽ§å·¥å…·
- **psutil**ï¼šç³»ç»Ÿç›‘æŽ§å·¥å…·

### ðŸ“‚ å…·ä½“æ“ä½œï¼š
```bash
# å‡çº§pip (åŒ…ç®¡ç†å™¨)
pip install --upgrade pip
# ðŸ‘€ ä½œç”¨ï¼šç¡®ä¿pipæ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œé¿å…å®‰è£…åŒ…æ—¶å‡ºé”™

# å®‰è£…transformers
pip install transformers>=4.36.0
# ðŸ‘€ ä½œç”¨ï¼š
#   â€¢ å®‰è£…HuggingFaceçš„transformersåº“
#   â€¢ >=4.36.0 ç¡®ä¿ä¸ŽPython 3.12å…¼å®¹
#   â€¢ è¿™æ˜¯ä¼ ç»Ÿæ–¹æ³•çš„æ ¸å¿ƒåº“

# å®‰è£…vLLM
pip install vllm
# ðŸ‘€ ä½œç”¨ï¼š
#   â€¢ å®‰è£…vLLMåº“ï¼ŒåŒ…å«PagedAttentionå®žçŽ°
#   â€¢ è¿™æ˜¯æˆ‘ä»¬è¦å¯¹æ¯”çš„æ–°æ–¹æ³•
#   â€¢ å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿä¸‹è½½å’Œç¼–è¯‘

# å®‰è£…ç›‘æŽ§å·¥å…·
pip install nvidia-ml-py3 psutil
# ðŸ‘€ ä½œç”¨ï¼š
#   â€¢ nvidia-ml-py3ï¼šç›´æŽ¥è®¿é—®GPUä¿¡æ¯
#   â€¢ psutilï¼šç›‘æŽ§ç³»ç»Ÿèµ„æºä½¿ç”¨

# å®‰è£…å…¶ä»–å·¥å…·
pip install matplotlib pandas tqdm
# ðŸ‘€ ä½œç”¨ï¼šæ•°æ®åˆ†æžå’Œå¯è§†åŒ–å·¥å…·ï¼ˆå¯é€‰ï¼‰
```

---

## ç¬¬å››æ­¥ï¼šéªŒè¯å®‰è£…
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
ç¡®è®¤æ‰€æœ‰åŒ…éƒ½æ­£ç¡®å®‰è£…äº†ï¼ŒçŽ¯å¢ƒæ­å»ºæˆåŠŸ

### ðŸ“‚ å…·ä½“æ“ä½œï¼š
```bash
# æ£€æŸ¥PyTorch
python -c "import torch; print('âœ… PyTorch:', torch.__version__)"
# ðŸ‘€ ä½œç”¨ï¼šç¡®è®¤PyTorchå¯ä»¥æ­£å¸¸å¯¼å…¥ï¼Œæ˜¾ç¤ºç‰ˆæœ¬å·

# æ£€æŸ¥Transformers
python -c "import transformers; print('âœ… Transformers:', transformers.__version__)"
# ðŸ‘€ ä½œç”¨ï¼šç¡®è®¤transformerså®‰è£…æˆåŠŸ

# æ£€æŸ¥vLLM
python -c "import vllm; print('âœ… vLLM:', vllm.__version__)"
# ðŸ‘€ ä½œç”¨ï¼šç¡®è®¤vLLMå®‰è£…æˆåŠŸ

# æ£€æŸ¥CUDA
python -c "import torch; print('âœ… CUDAå¯ç”¨:', torch.cuda.is_available()); print('âœ… GPUæ•°é‡:', torch.cuda.device_count())"
# ðŸ‘€ ä½œç”¨ï¼šç¡®è®¤GPUå¯ä»¥è¢«PyTorchè¯†åˆ«
```

### ðŸŽ‰ æˆåŠŸçš„è¾“å‡ºåº”è¯¥åƒè¿™æ ·ï¼š
```
âœ… PyTorch: 2.7.1
âœ… Transformers: 4.37.2
âœ… vLLM: 0.3.0
âœ… CUDAå¯ç”¨: True
âœ… GPUæ•°é‡: 1
```

---

## ç¬¬äº”æ­¥ï¼šåˆ›å»ºå†…å­˜ç›‘æŽ§è„šæœ¬
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
å†™ä¸€ä¸ªPythonè„šæœ¬æ¥ç›‘æŽ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ

### ðŸ“ åœ¨å“ªé‡Œè¿è¡Œï¼š
```bash
# ç¡®ä¿ä½ åœ¨è¿™ä¸ªç›®å½•ï¼š
pwd
# åº”è¯¥æ˜¾ç¤ºï¼š/home/cxxxxx/vllm-learning/experiments/paged-attention-test

# ç¡®ä¿è™šæ‹ŸçŽ¯å¢ƒå·²æ¿€æ´»ï¼š
source venv-paged-test/bin/activate
# å‘½ä»¤è¡Œå‰é¢åº”è¯¥æ˜¾ç¤ºï¼š(venv-paged-test)
```

### ðŸ” ä¸ºä»€ä¹ˆéœ€è¦ç›‘æŽ§ï¼š
- **å¯¹æ¯”å…³é”®**ï¼šçœ‹ä¸¤ç§æ–¹æ³•çš„å†…å­˜ä½¿ç”¨å·®å¼‚
- **å®žæ—¶è§‚å¯Ÿ**ï¼šäº†è§£å†…å­˜ä½¿ç”¨çš„å˜åŒ–æ¨¡å¼
- **å®šé‡åˆ†æž**ï¼šç”¨æ•°æ®è¯æ˜ŽPagedAttentionçš„ä¼˜åŠ¿

### ðŸ“‚ åˆ›å»ºè„šæœ¬ï¼š
```bash
cat > scripts/memory_monitor.py << 'EOF'
import torch
import time
from datetime import datetime

class MemoryMonitor:
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        if self.gpu_available:
            try:
                import nvidia_ml_py3 as nvml
                nvml.nvmlInit()
                self.nvml = nvml
                self.device_count = nvml.nvmlDeviceGetCount()
                self.nvml_available = True
                print("âœ… ä½¿ç”¨NVMLè¿›è¡ŒGPUç›‘æŽ§")
            except ImportError:
                print("âš ï¸  NVMLä¸å¯ç”¨ï¼Œä½¿ç”¨PyTorchè¿›è¡ŒGPUç›‘æŽ§")
                self.nvml_available = False
        else:
            print("âŒ GPUä¸å¯ç”¨")
    
    def get_gpu_memory_torch(self):
        """ä½¿ç”¨torchèŽ·å–GPUå†…å­˜"""
        if not self.gpu_available:
            return {"error": "No GPU available"}
        
        memory_info = {}
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            
            memory_info[f"GPU_{i}"] = {
                "total": total,
                "allocated": allocated,
                "cached": cached,
                "free": total - cached,
                "utilization": (allocated / total) * 100 if total > 0 else 0
            }
        return memory_info
    
    def get_gpu_memory_nvml(self):
        """ä½¿ç”¨NVMLèŽ·å–GPUå†…å­˜"""
        if not self.gpu_available or not self.nvml_available:
            return self.get_gpu_memory_torch()
            
        memory_info = {}
        for i in range(self.device_count):
            handle = self.nvml.nvmlDeviceGetHandleByIndex(i)
            memory = self.nvml.nvmlDeviceGetMemoryInfo(handle)
            
            memory_info[f"GPU_{i}"] = {
                "total": memory.total / 1024**3,
                "used": memory.used / 1024**3,
                "free": memory.free / 1024**3,
                "utilization": (memory.used / memory.total) * 100
            }
        return memory_info
    
    def get_gpu_memory(self):
        """èŽ·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.nvml_available:
            return self.get_gpu_memory_nvml()
        else:
            return self.get_gpu_memory_torch()
    
    def print_memory_status(self, label=""):
        """æ‰“å°å†…å­˜çŠ¶æ€"""
        print(f"\n=== {label} ===")
        print(f"æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        
        # GPUå†…å­˜
        gpu_memory = self.get_gpu_memory()
        if "error" not in gpu_memory:
            for gpu_id, mem in gpu_memory.items():
                if "used" in mem:
                    print(f"ðŸ”¥ {gpu_id}: {mem['used']:.2f}GB / {mem['total']:.2f}GB ({mem['utilization']:.1f}%)")
                else:
                    print(f"ðŸ”¥ {gpu_id}: {mem['allocated']:.2f}GB allocated / {mem['total']:.2f}GB total")
        else:
            print("âŒ GPUä¿¡æ¯èŽ·å–å¤±è´¥")
        
        print("-" * 50)

if __name__ == "__main__":
    monitor = MemoryMonitor()
    monitor.print_memory_status("ç³»ç»ŸçŠ¶æ€æ£€æŸ¥")
    
    # æµ‹è¯•GPUå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"ðŸ“Š GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
EOF
```

### ðŸŽ¯ è¿™ä¸ªè„šæœ¬è®©æˆ‘ä»¬èƒ½å¤Ÿï¼š
- åœ¨åŠ è½½æ¨¡åž‹å‰åŽå¯¹æ¯”å†…å­˜ä½¿ç”¨
- å¤„ç†ä¸åŒè¯·æ±‚æ—¶è§‚å¯Ÿå†…å­˜å˜åŒ–
- é‡åŒ–ä¸¤ç§æ–¹æ³•çš„å†…å­˜æ•ˆçŽ‡å·®å¼‚

---

## ç¬¬å…­æ­¥ï¼šæµ‹è¯•ç›‘æŽ§è„šæœ¬
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
è¿è¡Œåˆšåˆ›å»ºçš„ç›‘æŽ§è„šæœ¬ï¼Œç¡®ä¿å®ƒèƒ½æ­£å¸¸å·¥ä½œ

### ðŸ“ åœ¨å“ªé‡Œè¿è¡Œï¼š
```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
pwd
# åº”è¯¥æ˜¾ç¤ºï¼š/home/xxxxx/vllm-learning/experiments/paged-attention-test

# ç¡®ä¿è™šæ‹ŸçŽ¯å¢ƒå·²æ¿€æ´»
source venv-paged-test/bin/activate

# ä»Žé¡¹ç›®æ ¹ç›®å½•è¿è¡Œè„šæœ¬
python scripts/memory_monitor.py
```
**æ³¨æ„**ï¼šè„šæœ¬æ–‡ä»¶åœ¨ `scripts/` æ–‡ä»¶å¤¹é‡Œï¼Œä½†æˆ‘ä»¬ä»Žé¡¹ç›®æ ¹ç›®å½•è¿è¡Œå®ƒ

### ðŸŽ‰ æ­£å¸¸è¾“å‡ºåº”è¯¥åƒè¿™æ ·ï¼š
```
=== ç³»ç»ŸçŠ¶æ€æ£€æŸ¥ ===
æ—¶é—´: 20:30:15
ðŸ”¥ GPU_0: 0.56GB / 24.00GB (2.3%)
--------------------------------------------------
âœ… CUDAå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: 1
ðŸ“Š GPU 0: NVIDIA GeForce RTX 4090
```

---

## ç¬¬ä¸ƒæ­¥ï¼šåˆ›å»ºä¼ ç»Ÿæ–¹æ³•æµ‹è¯•è„šæœ¬
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
å†™ä¸€ä¸ªè„šæœ¬æ¥æµ‹è¯•ä¼ ç»ŸTransformersåº“çš„å†…å­˜ä½¿ç”¨æ–¹å¼

### ðŸ” ä¼ ç»Ÿæ–¹æ³•çš„ç‰¹ç‚¹ï¼š
- **é€ä¸ªå¤„ç†**ï¼šä¸€æ¬¡åªèƒ½å¤„ç†ä¸€ä¸ªè¯·æ±‚
- **å›ºå®šåˆ†é…**ï¼šæŒ‰æœ€å¤§å¯èƒ½é•¿åº¦åˆ†é…å†…å­˜
- **å†…å­˜æµªè´¹**ï¼šçŸ­æ–‡æœ¬ä¹Ÿå ç”¨é•¿æ–‡æœ¬çš„å†…å­˜ç©ºé—´

### ðŸ“‚ åˆ›å»ºè„šæœ¬ï¼š
```bash
cat > scripts/test_traditional.py << 'EOF'
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from memory_monitor import MemoryMonitor

def test_traditional_method():
    """æµ‹è¯•ä¼ ç»ŸTransformersæ–¹æ³•"""
    monitor = MemoryMonitor()
    
    print("ðŸ”„ å¼€å§‹æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•...")
    monitor.print_memory_status("åˆå§‹çŠ¶æ€")
    
    # ä½¿ç”¨GPT-2æ¨¡åž‹è¿›è¡Œæµ‹è¯•
    model_name = "gpt2"
    
    print("ðŸ“¥ æ­£åœ¨åŠ è½½ä¼ ç»ŸTransformersæ¨¡åž‹...")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°GPU
        if torch.cuda.is_available():
            model = model.to("cuda")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        monitor.print_memory_status("æ¨¡åž‹åŠ è½½å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # å‡†å¤‡æµ‹è¯•æ•°æ® - ä¸åŒé•¿åº¦çš„æ–‡æœ¬
    test_inputs = [
        "Hello!",  # çŸ­æ–‡æœ¬
        "How are you doing today? I hope everything is going well.",  # ä¸­ç­‰é•¿åº¦
        "Tell me a story about artificial intelligence and machine learning. " * 10,  # é•¿æ–‡æœ¬
        "Thanks!"  # çŸ­æ–‡æœ¬
    ]
    
    print(f"ðŸ“ å¤„ç† {len(test_inputs)} ä¸ªä¸åŒé•¿åº¦çš„è¾“å…¥...")
    
    total_time = 0
    for i, input_text in enumerate(test_inputs):
        print(f"\nðŸ“„ å¤„ç†è¾“å…¥ {i+1}: é•¿åº¦ {len(input_text)} å­—ç¬¦")
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(
                input_text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            
            # ç§»åŠ¨åˆ°GPU
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                start_time = time.time()
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )
                generation_time = time.time() - start_time
                total_time += generation_time
            
            # è§£ç è¾“å‡º
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
            print(f"ðŸ“Š è¾“å‡ºé•¿åº¦: {len(generated_text)} å­—ç¬¦")
            
            monitor.print_memory_status(f"å¤„ç†å®Œè¾“å…¥ {i+1}")
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            time.sleep(1)  # ç­‰å¾…å†…å­˜é‡Šæ”¾
                
        except Exception as e:
            print(f"âŒ å¤„ç†è¾“å…¥ {i+1} æ—¶å‡ºé”™: {e}")
    
    print(f"\nâœ… ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•å®Œæˆ!")
    print(f"ðŸ•’ æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"ðŸ“ˆ å¹³å‡æ¯ä¸ªè¯·æ±‚: {total_time/len(test_inputs):.2f}ç§’")
    print(f"ðŸ’¡ è§‚å¯Ÿè¦ç‚¹: æ¯ä¸ªè¯·æ±‚éƒ½éœ€è¦å•ç‹¬å¤„ç†ï¼Œå†…å­˜ä½¿ç”¨ä¼šæ³¢åŠ¨")

if __name__ == "__main__":
    test_traditional_method()
EOF
```

### ðŸ”§ å¦‚æžœé‡åˆ°accelerateç›¸å…³é”™è¯¯ï¼š
```bash
# å®‰è£…ç¼ºå¤±çš„ä¾èµ–
pip install accelerate

# é‡æ–°è¿è¡Œæµ‹è¯•
python scripts/test_traditional.py
```

### ðŸŽ¯ é€šè¿‡è¿™ä¸ªæµ‹è¯•æˆ‘ä»¬èƒ½è§‚å¯Ÿåˆ°ï¼š
- æ¯æ¬¡å¤„ç†æ—¶å†…å­˜å¦‚ä½•åˆ†é…
- ä¸åŒé•¿åº¦æ–‡æœ¬çš„å†…å­˜ä½¿ç”¨æ˜¯å¦ä¸åŒ
- ä¼ ç»Ÿæ–¹æ³•çš„å†…å­˜ä½¿ç”¨æ¨¡å¼

---

## ç¬¬å…«æ­¥ï¼šåˆ›å»ºvLLMæµ‹è¯•è„šæœ¬
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
å†™ä¸€ä¸ªè„šæœ¬æ¥æµ‹è¯•vLLMåº“çš„PagedAttentionå†…å­˜ä½¿ç”¨æ–¹å¼

### ðŸ” PagedAttentionçš„ç‰¹ç‚¹ï¼š
- **æ‰¹é‡å¤„ç†**ï¼šå¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚
- **åˆ†é¡µåˆ†é…**ï¼šæŒ‰å®žé™…éœ€è¦åˆ†é…å†…å­˜é¡µé¢
- **é«˜æ•ˆåˆ©ç”¨**ï¼šé¿å…å†…å­˜æµªè´¹å’Œç¢Žç‰‡åŒ–

### ðŸ“‚ åˆ›å»ºè„šæœ¬ï¼š
```bash
cat > scripts/test_vllm.py << 'EOF'
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import time
from memory_monitor import MemoryMonitor

def test_vllm_method():
    """æµ‹è¯•vLLM PagedAttentionæ–¹æ³•"""
    monitor = MemoryMonitor()
    
    print("ðŸ”„ å¼€å§‹æµ‹è¯•vLLMæ–¹æ³•...")
    monitor.print_memory_status("åˆå§‹çŠ¶æ€")
    
    try:
        from vllm import LLM, SamplingParams
        print("âœ… vLLMå¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âŒ vLLMå¯¼å…¥å¤±è´¥: {e}")
        print("ðŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…vLLM: pip install vllm")
        return
    
    # åŠ è½½æ¨¡åž‹
    print("ðŸ“¥ æ­£åœ¨åŠ è½½vLLMæ¨¡åž‹...")
    try:
        llm = LLM(
            model="gpt2",
            trust_remote_code=True,
            max_model_len=512,
            gpu_memory_utilization=0.8  # ä½¿ç”¨80%çš„GPUå†…å­˜
        )
        monitor.print_memory_status("vLLMæ¨¡åž‹åŠ è½½å®Œæˆ")
    except Exception as e:
        print(f"âŒ vLLMæ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=0.7,
        max_tokens=50,
        top_p=0.95
    )
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆä¸Žä¼ ç»Ÿæ–¹æ³•ç›¸åŒï¼‰
    test_inputs = [
        "Hello!",  # çŸ­æ–‡æœ¬
        "How are you doing today? I hope everything is going well.",  # ä¸­ç­‰é•¿åº¦  
        "Tell me a story about artificial intelligence and machine learning. " * 10,  # é•¿æ–‡æœ¬
        "Thanks!"  # çŸ­æ–‡æœ¬
    ]
    
    print(f"ðŸ“ å¤„ç† {len(test_inputs)} ä¸ªä¸åŒé•¿åº¦çš„è¾“å…¥...")
    print("ðŸš€ vLLMçš„ä¼˜åŠ¿ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰è¯·æ±‚ï¼")
    
    try:
        # æ‰¹é‡å¤„ç†æ‰€æœ‰è¾“å…¥ - è¿™æ˜¯vLLMçš„æ ¸å¿ƒä¼˜åŠ¿
        start_time = time.time()
        outputs = llm.generate(test_inputs, sampling_params)
        total_time = time.time() - start_time
        
        monitor.print_memory_status("æ‰¹é‡ç”Ÿæˆå®Œæˆ")
        
        # æ˜¾ç¤ºç»“æžœ
        for i, output in enumerate(outputs):
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"\nðŸ“„ è¾“å…¥ {i+1}: '{prompt[:50]}...'")
            print(f"ðŸ“Š åŽŸæ–‡é•¿åº¦: {len(prompt)} å­—ç¬¦")
            print(f"ðŸ“Š ç”Ÿæˆé•¿åº¦: {len(generated_text)} å­—ç¬¦")
        
        print(f"\nâœ… vLLMæ–¹æ³•æµ‹è¯•å®Œæˆ!")
        print(f"ðŸ•’ æ€»ç”Ÿæˆæ—¶é—´: {total_time:.2f}ç§’")
        print(f"ðŸ“ˆ å¹³å‡æ¯ä¸ªè¯·æ±‚: {total_time/len(test_inputs):.2f}ç§’")
        print(f"ðŸ”¥ æ ¸å¿ƒä¼˜åŠ¿: æ‰¹é‡å¤„ç†ï¼Œå†…å­˜ä½¿ç”¨æ›´ç¨³å®šï¼Œæ”¯æŒPagedAttention!")
        
    except Exception as e:
        print(f"âŒ vLLMç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    test_vllm_method()
EOF
```

### ðŸŽ¯ é€šè¿‡è¿™ä¸ªæµ‹è¯•æˆ‘ä»¬èƒ½è§‚å¯Ÿåˆ°ï¼š
- æ‰¹é‡å¤„ç†çš„æ•ˆçŽ‡ä¼˜åŠ¿
- å†…å­˜ä½¿ç”¨çš„ç¨³å®šæ€§
- PagedAttentionçš„å®žé™…æ•ˆæžœ

---

## ç¬¬ä¹æ­¥ï¼šåˆ›å»ºè¿è¡Œè„šæœ¬
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
å†™ä¸€ä¸ªä¸»æŽ§è„šæœ¬ï¼Œè‡ªåŠ¨è¿è¡Œæ•´ä¸ªå®žéªŒæµç¨‹

### ðŸ“‚ åˆ›å»ºè„šæœ¬ï¼š
```bash
cat > run_experiment.py << 'EOF'
#!/usr/bin/env python3
import subprocess
import sys
import os
import time

def check_environment():
    """æ£€æŸ¥å®žéªŒçŽ¯å¢ƒ"""
    print("ðŸ” æ£€æŸ¥å®žéªŒçŽ¯å¢ƒ...")
    
    # æ£€æŸ¥Pythonç‰ˆæœ¬
    print(f"ðŸ Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥è™šæ‹ŸçŽ¯å¢ƒ
    if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):
        print("âœ… åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿è¡Œ")
    else:
        print("âš ï¸  å»ºè®®åœ¨è™šæ‹ŸçŽ¯å¢ƒä¸­è¿è¡Œ")
    
    # æ£€æŸ¥å¿…è¦æ¨¡å—
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ðŸ“Š GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformersç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformersæœªå®‰è£…")
        return False
    
    try:
        import vllm
        print(f"âœ… vLLMç‰ˆæœ¬: {vllm.__version__}")
    except ImportError:
        print("âŒ vLLMæœªå®‰è£…")
        return False
    
    return True

def run_experiment():
    """è¿è¡Œå¯¹æ¯”å®žéªŒ"""
    print("\n" + "=" * 60)
    print("ðŸ§ª PagedAttention å†…å­˜åˆ†é…å¯¹æ¯”å®žéªŒ")
    print("ðŸŽ¯ ç›®æ ‡ï¼šç†è§£PagedAttentionå¦‚ä½•æé«˜å†…å­˜åˆ©ç”¨çŽ‡")
    print("=" * 60)
    
    if not check_environment():
        print("âŒ çŽ¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…å¿…è¦çš„ä¾èµ–")
        return
    
    script_dir = "scripts"
    
    # æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•
    print("\n" + "="*50)
    print("ðŸ”¬ ç¬¬ä¸€é˜¶æ®µï¼šä¼ ç»ŸTransformersæ–¹æ³•æµ‹è¯•")
    print("ðŸ’¡ ç‰¹ç‚¹ï¼šé€ä¸ªå¤„ç†è¯·æ±‚ï¼ŒæŒ‰æœ€å¤§å¯èƒ½é•¿åº¦åˆ†é…å†…å­˜")
    print("="*50)
    
    try:
        print("â³ æ­£åœ¨è¿è¡Œä¼ ç»Ÿæ–¹æ³•æµ‹è¯•...")
        result = subprocess.run([sys.executable, f"{script_dir}/test_traditional.py"], 
                              cwd=os.getcwd())
        if result.returncode == 0:
            print("\nâœ… ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•å®Œæˆ")
        else:
            print(f"\nâŒ ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•å¤±è´¥ï¼Œè¿”å›žç : {result.returncode}")
    except Exception as e:
        print(f"âŒ ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•å‡ºé”™: {e}")
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    print("\n" + "="*50)
    input("â¸ï¸  æŒ‰Enteré”®ç»§ç»­æµ‹è¯•vLLM PagedAttentionæ–¹æ³•...")
    
    # æµ‹è¯•vLLMæ–¹æ³•
    print("\n" + "="*50)
    print("ðŸ”¬ ç¬¬äºŒé˜¶æ®µï¼švLLM PagedAttentionæ–¹æ³•æµ‹è¯•")
    print("ðŸ’¡ ç‰¹ç‚¹ï¼šæ‰¹é‡å¤„ç†ï¼Œåˆ†é¡µç®¡ç†KV-Cacheï¼ŒæŒ‰éœ€åˆ†é…å†…å­˜")
    print("="*50)
    
    try:
        print("â³ æ­£åœ¨è¿è¡ŒvLLMæ–¹æ³•æµ‹è¯•...")
        result = subprocess.run([sys.executable, f"{script_dir}/test_vllm.py"], 
                              cwd=os.getcwd())
        if result.returncode == 0:
            print("\nâœ… vLLMæ–¹æ³•æµ‹è¯•å®Œæˆ")
        else:
            print(f"\nâŒ vLLMæ–¹æ³•æµ‹è¯•å¤±è´¥ï¼Œè¿”å›žç : {result.returncode}")
    except Exception as e:
        print(f"âŒ vLLMæ–¹æ³•æµ‹è¯•å‡ºé”™: {e}")
    
    # å®žéªŒæ€»ç»“
    print("\n" + "=" * 60)
    print("ðŸŽ‰ å®žéªŒå®Œæˆï¼PagedAttentionåŽŸç†è§£æž")
    print("=" * 60)
    print("\nðŸ” å…³é”®è§‚å¯Ÿç‚¹:")
    print("1. ðŸ’¾ GPUå†…å­˜ä½¿ç”¨æ¨¡å¼:")
    print("   â€¢ ä¼ ç»Ÿæ–¹æ³•ï¼šæ¯ä¸ªåºåˆ—æŒ‰æœ€å¤§é•¿åº¦åˆ†é…ï¼Œå­˜åœ¨å†…å­˜æµªè´¹")
    print("   â€¢ PagedAttentionï¼šæŒ‰å®žé™…éœ€è¦åˆ†é¡µåˆ†é…ï¼Œå†…å­˜åˆ©ç”¨çŽ‡æ›´é«˜")
    print("\n2. ðŸ“Š å†…å­˜ä½¿ç”¨ç¨³å®šæ€§:")
    print("   â€¢ ä¼ ç»Ÿæ–¹æ³•ï¼šå†…å­˜ä½¿ç”¨éšè¾“å…¥é•¿åº¦æ³¢åŠ¨å¤§")
    print("   â€¢ PagedAttentionï¼šå†…å­˜ä½¿ç”¨æ›´å¹³ç¨³ï¼Œé¢„æµ‹æ€§æ›´å¥½")
    print("\n3. âš¡ å¤„ç†æ•ˆçŽ‡:")
    print("   â€¢ ä¼ ç»Ÿæ–¹æ³•ï¼šé€ä¸ªå¤„ç†è¯·æ±‚")
    print("   â€¢ PagedAttentionï¼šæ”¯æŒé«˜æ•ˆæ‰¹é‡å¤„ç†")
    print("\nðŸ’¡ PagedAttentionçš„æ ¸å¿ƒåˆ›æ–°:")
    print("ðŸ”¹ å°†KV-Cacheåˆ†å‰²æˆå›ºå®šå¤§å°çš„é¡µé¢(pages)")
    print("ðŸ”¹ æŒ‰éœ€åˆ†é…é¡µé¢ï¼Œé¿å…é¢„å…ˆåˆ†é…å¤§å—è¿žç»­å†…å­˜")
    print("ðŸ”¹ æ”¯æŒåŠ¨æ€çš„å†…å­˜ç®¡ç†å’Œæ›´å¥½çš„å†…å­˜ç¢Žç‰‡å¤„ç†")
    print("ðŸ”¹ ä½¿å¾—å˜é•¿åºåˆ—çš„æ‰¹å¤„ç†æˆä¸ºå¯èƒ½")
    print("\nðŸŽ¯ å®žé™…æ„ä¹‰:")
    print("ðŸ“ˆ åœ¨ç›¸åŒç¡¬ä»¶ä¸Šæ”¯æŒæ›´å¤§çš„æ‰¹å¤„ç†å¤§å°")
    print("ðŸ’° é™ä½ŽæŽ¨ç†æœåŠ¡çš„ç¡¬ä»¶æˆæœ¬")
    print("ðŸš€ æé«˜æ¨¡åž‹æœåŠ¡çš„åžåé‡")

if __name__ == "__main__":
    run_experiment()
EOF

# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x run_experiment.py
chmod +x scripts/*.py
```

### ðŸŽ¯ è¿™ä¸ªè„šæœ¬çš„ä»·å€¼ï¼š
- **è‡ªåŠ¨åŒ–**ï¼šä¸€é”®è¿è¡Œæ‰€æœ‰æµ‹è¯•
- **ç”¨æˆ·å‹å¥½**ï¼šæ¸…æ™°çš„æç¤ºå’Œè¯´æ˜Ž
- **æ•™è‚²æ€§**ï¼šè§£é‡Šæ¯ä¸ªçŽ°è±¡èƒŒåŽçš„åŽŸç†

---

## ç¬¬åæ­¥ï¼šè¿è¡Œå®žéªŒ
### ðŸŽ¯ è¿™ä¸€æ­¥åœ¨åšä»€ä¹ˆï¼š
æ‰§è¡Œå®Œæ•´çš„å¯¹æ¯”å®žéªŒï¼Œè§‚å¯Ÿä¸¤ç§æ–¹æ³•çš„å·®å¼‚

### ðŸ“ åœ¨å“ªé‡Œè¿è¡Œï¼š
```bash
# 1. ç¡®ä¿åœ¨æ­£ç¡®ç›®å½•
pwd
# å¿…é¡»æ˜¾ç¤ºï¼š/home/christina/vllm-learning/experiments/paged-attention-test

# 2. ç¡®ä¿è™šæ‹ŸçŽ¯å¢ƒå·²æ¿€æ´»
source venv-paged-test/bin/activate
# å‘½ä»¤è¡Œå‰é¢åº”è¯¥æ˜¾ç¤ºï¼š(venv-paged-test)

# 3. è¿è¡Œä¸»å®žéªŒè„šæœ¬
python run_experiment.py
```

### ðŸ—‚ï¸ é‡è¦çš„ç›®å½•ç»“æž„ï¼š
```
/home/cxxxxx/vllm-learning/experiments/paged-attention-test/  â† ä½ åœ¨è¿™é‡Œè¿è¡Œ
â”œâ”€â”€ venv-paged-test/           â† è™šæ‹ŸçŽ¯å¢ƒ
â”œâ”€â”€ scripts/                   â† è„šæœ¬æ–‡ä»¶å­˜æ”¾å¤„
â”‚   â”œâ”€â”€ memory_monitor.py      â† ç›‘æŽ§è„šæœ¬
â”‚   â”œâ”€â”€ test_traditional.py    â† ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•
â”‚   â””â”€â”€ test_vllm.py          â† vLLMæ–¹æ³•æµ‹è¯•
â”œâ”€â”€ logs/                      â† æ—¥å¿—è¾“å‡º
â”œâ”€â”€ results/                   â† ç»“æžœæ–‡ä»¶
â””â”€â”€ run_experiment.py          â† ä¸»è¿è¡Œè„šæœ¬ï¼ˆåœ¨è¿™é‡Œï¼‰
```

### ðŸ”„ è„šæœ¬è°ƒç”¨å…³ç³»ï¼š
```
ä½ è¿è¡Œ: python run_experiment.py  (åœ¨æ ¹ç›®å½•)
   â†“
è‡ªåŠ¨è°ƒç”¨: python scripts/test_traditional.py
   â†“  
è‡ªåŠ¨è°ƒç”¨: python scripts/test_vllm.py
```

### ðŸŽ¯ ä½ å°†è§‚å¯Ÿåˆ°çš„å…³é”®å·®å¼‚ï¼š

#### ä¼ ç»Ÿæ–¹æ³•ç‰¹å¾ï¼š
- ðŸ“Š å†…å­˜ä½¿ç”¨ï¼šéšè¾“å…¥é•¿åº¦æ³¢åŠ¨
- â±ï¸ å¤„ç†æ–¹å¼ï¼šé€ä¸ªå¤„ç†è¯·æ±‚
- ðŸ’¾ å†…å­˜åˆ†é…ï¼šæŒ‰æœ€å¤§é•¿åº¦é¢„åˆ†é…

#### PagedAttentionç‰¹å¾ï¼š
- ðŸ“Š å†…å­˜ä½¿ç”¨ï¼šæ›´ç¨³å®šå’Œé«˜æ•ˆ
- â±ï¸ å¤„ç†æ–¹å¼ï¼šæ‰¹é‡å¤„ç†
- ðŸ’¾ å†…å­˜åˆ†é…ï¼šæŒ‰éœ€åˆ†é¡µåˆ†é…

### ðŸŽ“ å®žéªŒçš„æ•™è‚²ä»·å€¼ï¼š
é€šè¿‡è¿™ä¸ªå®žéªŒï¼Œä½ å°†æ·±å…¥ç†è§£ï¼š
1. **PagedAttentionçš„å·¥ä½œåŽŸç†**
2. **ä¸ºä»€ä¹ˆå®ƒèƒ½æé«˜å†…å­˜åˆ©ç”¨çŽ‡**
3. **åœ¨å®žé™…åº”ç”¨ä¸­çš„ä¼˜åŠ¿**

---

## æ€»ç»“ï¼šæ¯ä¸€æ­¥çš„æ ¸å¿ƒç›®çš„

| æ­¥éª¤ | ä¸»è¦ç›®çš„ | å…³é”®äº§å‡º |
|------|----------|----------|
| 1-2 | çŽ¯å¢ƒå‡†å¤‡ | ç‹¬ç«‹çš„å®žéªŒçŽ¯å¢ƒ |
| 3-4 | ä¾èµ–å®‰è£… | å®Œæ•´çš„è½¯ä»¶æ ˆ |
| 5-6 | ç›‘æŽ§å·¥å…· | å†…å­˜ä½¿ç”¨è§‚å¯Ÿèƒ½åŠ› |
| 7-8 | æµ‹è¯•è„šæœ¬ | ä¸¤ç§æ–¹æ³•çš„å¯¹æ¯”æµ‹è¯• |
| 9-10 | å®žéªŒæ‰§è¡Œ | PagedAttentionåŽŸç†çš„ç›´è§‚ç†è§£ |

æ¯ä¸€æ­¥éƒ½æ˜¯ä¸ºäº†æœ€ç»ˆç›®æ ‡æœåŠ¡ï¼š**é€šè¿‡å®žé™…å¯¹æ¯”ï¼Œæ·±å…¥ç†è§£PagedAttentionçš„å·¥ä½œåŽŸç†å’Œä¼˜åŠ¿**ã€‚
